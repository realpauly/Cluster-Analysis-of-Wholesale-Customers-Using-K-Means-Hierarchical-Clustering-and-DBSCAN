# -*- coding: utf-8 -*-
"""clustering_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nmsdqr-Sl1VXI9UL1ZH6zdjjQNYB9r8q
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
# Load your dataset (replace 'your_data.csv' with the actual filename)
df = pd.read_csv("Wholesale customers data.csv")

# Standardize the dataset if not already done
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(scaled_data)
    wcss.append(kmeans.inertia_)

# Plot the Elbow Method results
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()
silhouette_scores = []
for i in range(2, 11):  # Silhouette score requires at least 2 clusters
    kmeans = KMeans(n_clusters=i, random_state=42)
    labels = kmeans.fit_predict(scaled_data)
    score = silhouette_score(scaled_data, labels)
    silhouette_scores.append(score)

# Plot the silhouette scores
plt.figure(figsize=(8, 5))
plt.plot(range(2, 11), silhouette_scores, marker='o')
plt.title('Silhouette Score Method')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.show()
# Set the optimal number of clusters (e.g., 3 here)
optimal_clusters = 3

# Fit K-means clustering
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
df['Cluster'] = kmeans.fit_predict(scaled_data)
from sklearn.decomposition import PCA

# Reduce dimensions for visualization
pca = PCA(n_components=2)
pca_data = pca.fit_transform(scaled_data)

# Add PCA results to the DataFrame for plotting
df['PCA1'] = pca_data[:, 0]
df['PCA2'] = pca_data[:, 1]

# Plot the clusters
plt.figure(figsize=(10, 7))
sns.scatterplot(data=df, x='PCA1', y='PCA2', hue='Cluster', palette='viridis', s=100)
plt.title('Clusters Visualization with PCA')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend(title='Cluster')
plt.show()
# Silhouette Score
silhouette_avg = silhouette_score(scaled_data, df['Cluster'])
print(f"Silhouette Score for {optimal_clusters} clusters: {silhouette_avg:.3f}")

# Within-Cluster Sum of Squares (WCSS)
print(f"Within-Cluster Sum of Squares (WCSS) for {optimal_clusters} clusters: {kmeans.inertia_:.3f}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.decomposition import PCA
# Load your dataset (replace 'your_data.csv' with the actual filename)
df = pd.read_csv("Wholesale customers data.csv")

# Standardize the dataset if not already done
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)
# Optimal clusters determined previously (e.g., 3 clusters)
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans_labels = kmeans.fit_predict(scaled_data)

# Evaluate K-means
kmeans_silhouette = silhouette_score(scaled_data, kmeans_labels)
print(f"K-means Silhouette Score: {kmeans_silhouette:.3f}")
# Generate a linkage matrix for the dendrogram
linked = linkage(scaled_data, method='ward')

# Plot dendrogram
plt.figure(figsize=(10, 7))
dendrogram(linked)
plt.title("Dendrogram for Hierarchical Clustering")
plt.xlabel("Samples")
plt.ylabel("Distance")
plt.show()
# Apply hierarchical clustering with the chosen number of clusters
agglo = AgglomerativeClustering(n_clusters=3)
agglo_labels = agglo.fit_predict(scaled_data)

# Evaluate hierarchical clustering
agglo_silhouette = silhouette_score(scaled_data, agglo_labels)
print(f"Hierarchical Clustering Silhouette Score: {agglo_silhouette:.3f}")
# Apply DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(scaled_data)

# Filter out noise points (-1 labels) for silhouette score calculation
dbscan_silhouette = silhouette_score(scaled_data[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1])
print(f"DBSCAN Silhouette Score (excluding noise): {dbscan_silhouette:.3f}")
# Reduce to 2D using PCA for visualization
pca = PCA(n_components=2)
pca_data = pca.fit_transform(scaled_data)

# Add clustering labels from each algorithm to a DataFrame
df_pca = pd.DataFrame(pca_data, columns=['PCA1', 'PCA2'])
df_pca['KMeans'] = kmeans_labels
df_pca['Agglomerative'] = agglo_labels
df_pca['DBSCAN'] = dbscan_labels

# Plotting each clustering result
fig, axs = plt.subplots(1, 3, figsize=(18, 6))
sns.scatterplot(data=df_pca, x='PCA1', y='PCA2', hue='KMeans', palette='viridis', ax=axs[0], legend='full')
axs[0].set_title("K-means Clustering")
sns.scatterplot(data=df_pca, x='PCA1', y='PCA2', hue='Agglomerative', palette='viridis', ax=axs[1], legend='full')
axs[1].set_title("Hierarchical Clustering")
sns.scatterplot(data=df_pca, x='PCA1', y='PCA2', hue='DBSCAN', palette='viridis', ax=axs[2], legend='full')
axs[2].set_title("DBSCAN Clustering")
plt.show()

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import dendrogram, linkage

# Step 1: Load the Dataset
df = pd.read_csv("Wholesale customers data.csv")

# Display the first few rows of the dataset to understand its structure
print("First few rows of the dataset:")
print(df.head())

# Generate descriptive statistics for the dataset
print("\nDescriptive Statistics:")
print(df.describe())

# Check for any missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Step 2: Data Preprocessing
# Check for missing values
print("Missing values:\n", df.isnull().sum())
# Normalize the data for clustering
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df.iloc[:, 2:])  # Exclude Channel and Region columns if required

# Step 3: Apply K-means Clustering
# Determine the optimal number of clusters using the Elbow method
wcss = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(df_scaled)
    wcss.append(kmeans.inertia_)

# Plot the Elbow curve
plt.figure(figsize=(8, 4))
plt.plot(range(1, 11), wcss, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.title('Elbow Method for Optimal K')
plt.show()

# Fit KMeans with the chosen number of clusters (e.g., 3 based on the Elbow curve)
optimal_k = 3
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
kmeans_labels = kmeans.fit_predict(df_scaled)

# Step 4: Visualize Clustering Results
df['Cluster'] = kmeans_labels
sns.pairplot(df, hue='Cluster', palette='viridis', markers=["o", "s", "D"])
plt.suptitle("K-means Clustering Results", y=1.02)
plt.show()

# Step 5: Evaluate Clustering Quality
silhouette_avg = silhouette_score(df_scaled, kmeans_labels)
print(f"Silhouette Score for K-means with {optimal_k} clusters: {silhouette_avg}")

# Step 6: Interpret and Analyze the Clusters
# Describe characteristics of each cluster
print("Cluster analysis:")
for cluster in range(optimal_k):
    print(f"\nCluster {cluster}")
    print(df[df['Cluster'] == cluster].describe())

# Step 7: Experiment with Other Clustering Algorithms

# Hierarchical Clustering (Agglomerative)
agg_cluster = AgglomerativeClustering(n_clusters=optimal_k)
agg_labels = agg_cluster.fit_predict(df_scaled)
df['Agg_Cluster'] = agg_labels

# Plot Dendrogram for hierarchical clustering
linked = linkage(df_scaled, 'ward')
plt.figure(figsize=(10, 5))
dendrogram(linked, truncate_mode='level', p=3)
plt.title("Hierarchical Clustering Dendrogram")
plt.show()

# DBSCAN Clustering
dbscan = DBSCAN(eps=1.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(df_scaled)
df['DBSCAN_Cluster'] = dbscan_labels

# Visualize DBSCAN results
sns.scatterplot(x=df_scaled[:, 0], y=df_scaled[:, 1], hue=dbscan_labels, palette="viridis", s=50)
plt.xlabel("Scaled Feature 1")
plt.ylabel("Scaled Feature 2")
plt.title("DBSCAN Clustering Results")
plt.legend(title='Cluster')
plt.show()

# Step 8: Reflect on Limitations and Challenges
# Print a summary for reflection in the report
print("\n--- Summary of Clustering Results ---")
print("K-means Silhouette Score:", silhouette_avg)
print("Other clustering algorithms like hierarchical and DBSCAN provide additional perspectives but may vary in cluster quality.")